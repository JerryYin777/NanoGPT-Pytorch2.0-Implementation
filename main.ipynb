{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53178c13-cb47-4180-aa1c-025b099e7e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# nanoGPT\n",
    "\n",
    "本教程基于 [nanoGPT](https://github.com/karpathy/nanoGPT) 进行中型 GPT 的训练和微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d2078-eb3f-45b4-ac2f-8c02bbacc13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 环境准备\n",
    "\n",
    "> 建议使用 [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0) 环境运行本教程。PyTorch 2.0 增加了 `torch.compile()`，通过编译的方式，仅用一行代码即可实现模型的稳定加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21f74b-efbf-488d-ad12-30b049481ac1",
   "metadata": {},
   "source": [
    "在终端中运行以下命令，可进行 PyTorch 2.0 的安装：\n",
    "\n",
    "```shell\n",
    "pip3 install --user --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418e8ec-875c-47e7-9045-0d961b67e87b",
   "metadata": {},
   "source": [
    "在终端中运行以下命令安装其他依赖：\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2deba-9434-430a-99fc-b7d1680e079f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型训练\n",
    "\n",
    "此过程默认复现 OpenAI 发布的最小 GPT-2 模型（即 124M 版本）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f72d8-769d-410b-b571-d407d99c21d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 数据准备\n",
    "\n",
    "模型训练使用来自 OpenAI 的 WebText 数据集（12.9G）。\n",
    "\n",
    "在终端中依次运行下列命令，将文档 tokenize 为一个简单的长一维索引数组：\n",
    "\n",
    "```shell\n",
    "cd data/openwebtext\n",
    "python prepare.py\n",
    "```\n",
    "\n",
    "这将生成两个文件：train.bin 和 val.bin，每个文件都包含一个代表 GPT-2 BPE token id 的 uint16 字节原始序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0c9df-d9af-4e43-84ac-b4f04d75fc99",
   "metadata": {},
   "source": [
    "> **注意：** `prepare.py` 脚本运行所需时间较长，我们已将生成好的 train.bin 和 val.bin 文件添加至 [openwebtext 数据集](https://openbayes.com/console/open-tutorials/datasets/5iZ23a5zCGe/1/overview)，以便在后续执行中快速载入数据。此容器已绑定 openwebtext 数据集至 `/openbayes/input/input0` 目录下。您可以跳过『数据准备』步骤直接进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1396c-e64b-4862-a2d8-3f8cf68f216c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 训练\n",
    "\n",
    "在终端中依次运行下列命令：\n",
    "\n",
    "```shell\n",
    "# 返回 /openbayes/home 目录\n",
    "cd /openbayes/home\n",
    "\n",
    "# 开始训练\n",
    "python train.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11380af3-672b-4042-9b21-8f995d2428ad",
   "metadata": {},
   "source": [
    "默认情况下，会定期向输出目录写入 checkpoints。只要有一个 checkpoints 后，就可以运行下列命令从模型中进行采样：\n",
    "\n",
    "```shell\n",
    "python sample.py\n",
    "```\n",
    "\n",
    "部分参数说明：\n",
    "- out_dir: checkpoints 保存路径，默认为 \"out\"\n",
    "- start: 生成样本的起始文本\n",
    "- num_samples: 生成的样本数量，默认为 10\n",
    "- max_new_tokens: 每个样本生成的最大 token 数量，默认为 500\n",
    "\n",
    "如图为训练 96000 个 step 后，模型生成的文本样例：\n",
    "![sample](show/sample_96000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d80dd-0da0-4fe7-bc97-0a4adaf52e41",
   "metadata": {},
   "source": [
    "## 模型训练 demo\n",
    "\n",
    "若没有足够的时间和资源，仍想体验模型训练过程，可参考此过程训练一个 toy model。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c123521-2429-4500-8b72-a930e5d55922",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "\n",
    "使用莎士比亚数据集。\n",
    "\n",
    "在终端中依次运行下列命令：\n",
    "\n",
    "```shell\n",
    "cd data/shakespeare\n",
    "python prepare.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b08be2-256e-4c1d-b403-5c1df856a6bd",
   "metadata": {},
   "source": [
    "> **注意：** 此容器已绑定了处理好的 [shakespeare 数据集](https://openbayes.com/console/open-tutorials/datasets/ykDI9iRlxZb/1/overview)至 `/openbayes/input/input1` 目录下。您可以跳过『数据准备』步骤直接进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2caf5-ba32-4187-ab7f-9a6ed669f447",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "在终端中依次运行下列命令：\n",
    "\n",
    "```shell\n",
    "# 返回 /openbayes/home 目录\n",
    "cd /openbayes/home\n",
    "\n",
    "# 开始训练\n",
    "python train.py \\\n",
    "       --dataset=shakespeare \\\n",
    "       --n_layer=4 \\\n",
    "       --n_head=4 \\\n",
    "       --n_embd=64 \\\n",
    "       --eval_iters=1 \\\n",
    "       --block_size=64 \\\n",
    "       --batch_size=8 \\\n",
    "       --max_iters=10000\n",
    "```\n",
    "\n",
    "部分参数说明：\n",
    "\n",
    "- out_dir: checkpoints 保存路径，默认为 \"out\"\n",
    "- dataset: 数据集\n",
    "- n_layer: 神经网络的层数\n",
    "- n_head: 注意力头的数量\n",
    "- n_embd: 词向量、位置向量以及内部特征向量的维数\n",
    "- eval_iters: 评估迭代次数\n",
    "- block_size: 块大小\n",
    "- batch_size: 批大小\n",
    "- max_iters: 训练迭代次数\n",
    "- device: 默认为 \"cuda\"，若在 CPU 上运行则需设置为 \"cpu\"\n",
    "- compile: 使用 PyTorch2.0 中的 `torch.compile()` 加速模型训练，默认为 \"True\"，若在 PyTorch < 2.0 环境中运行，则需设置为 \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b576d-a173-45b1-917b-e59cb9a9455f",
   "metadata": {},
   "source": [
    "![compare](show/compare.jpg)\n",
    "> **注意：** 在 PyTorch2.0 环境中使用 torch.compile() 进行训练相比在 PyTorch1.13 环境中，单次迭代所需的时间将减少一半以上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c339e1-1ccc-4993-8eee-4c7907d4ecaf",
   "metadata": {},
   "source": [
    "## 微调\n",
    "\n",
    "基于 GPT 模型进行微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01fa80-18d9-43b1-bcac-b5c0a0ff3302",
   "metadata": {
    "tags": []
   },
   "source": [
    "本教程中使用莎士比亚数据集，基于 `gpt2-xl` 模型进行微调。\n",
    "\n",
    "可参考『模型训练 demo』进行数据准备，然后在终端中运行下列命令：\n",
    "\n",
    "```shell\n",
    "python train.py finetune_shakespeare.py\n",
    "```\n",
    "\n",
    "部分参数说明：\n",
    "- init_from: 初始化模型，可选参数：'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'，默认为 'gpt2-xl'。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbaca9-8ce0-4c73-9173-5cc70f4de564",
   "metadata": {},
   "source": [
    "### 模型推理\n",
    "\n",
    "在终端中运行以下命令即可生成莎士比亚风格的文本：\n",
    "\n",
    "```shell\n",
    "python sample.py --out_dir=out-shakespeare\n",
    "```\n",
    "\n",
    "如图为微调 1000 个 step 后，模型生成的文本样例：\n",
    "\n",
    "![finetune_1000](show/finetune_1000.png)\n",
    "\n",
    "通过 `start` 参数，控制模型以 “To be or not to be” 为开头生成后续文本：\n",
    "\n",
    "```shell\n",
    "python sample.py --out_dir=out-shakespeare --start=\"To be or not to be\"\n",
    "```\n",
    "\n",
    "![finetune_start](show/finetune_start.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b959460-b75e-4e04-a797-d4f881d5d4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
